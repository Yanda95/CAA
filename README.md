<div align="center">

# ğŸ§ CAA: Chat-Audio Attacks

**ACL 2025 Findings**  
ğŸ“„ *Who Can Withstand Chat-Audio Attacks? An Evaluation Benchmark for Large Language Models*

[![Paper](https://img.shields.io/badge/Paper-Findings%20of%20ACL%202025-blue)](https://arxiv.org/pdf/2411.14842)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

</div>

---

## ğŸ§  Overview

Large Language Models (LLMs) are increasingly deployed in **voice-based interactive systems**, yet little is known about their **robustness to audio attacks**.  
**CAA (Chat-Audio Attacks)** is the first benchmark that systematically evaluates LLMs against **diverse audio-based adversarial threats**, covering both **content-based** and **emotion-based** manipulations.

CAA offers:
- âš”ï¸ **1760 high-quality audio adversarial samples**
- ğŸ¯ Attacks on **response quality** rather than just transcription
- ğŸ“Š Evaluation on popular LLMs including **GPT-4o**, **Qwen2-Audio**, **SALMONN**, and more

---

## ğŸ“¥ Benchmark Download

You can download the CAA benchmark from the following link:

ğŸ”— [Google Drive (CAA)](https://drive.google.com/file/d/1kUkUDzYmRDsSjc69F_mA3IEXZ1RP1G4o/view?usp=drive_link)

---
